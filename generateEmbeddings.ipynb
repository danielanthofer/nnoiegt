{
 "metadata": {
  "name": "",
  "signature": "sha256:8baf294ad474cf87f0fd2a245c53256576097191e6fe3ee6437d38715c6273db"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- 32-imensional embeddings for German regular words\n",
      "    - no need for large embeddings because meaning of the word is only important in rare, specific cases\n",
      "- 24-dimensional embeddings for POS tags and dependency labels"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "ATTENTION: same intermediate file names for both embeddings"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Embeddings for Regular German Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# don't forget '/' at the end\n",
      "path_to_GloVe = \"external/GloVe-master/build/\"\n",
      "\n",
      "path_to_data = \"data/embeddings128/\"\n",
      "corpus_for_embeddings_regular_words = path_to_data + \"embeddings_corpus_regular_words.txt\"\n",
      "vocabulary_count = path_to_data + \"embeddings_vocab_count.txt\"\n",
      "vocabulary_cooccur = path_to_data + \"embeddings_vocab_cooccur.bin\"\n",
      "vocabulary_shuffle = path_to_data + \"embeddings_vocab_cooccur_shuffle.bin\"\n",
      "vocabulary_embeddings_regular_words = path_to_data + \"embeddings_regular_words\"\n",
      "\n",
      "embedding_dimensions_regular_words = 128\n",
      "\n",
      "import subprocess, shlex, re, os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**From GloVe README**\n",
      "\n",
      "1. vocab_count\n",
      "Constructs unigram counts from a corpus, and optionally thresholds the resulting vocabulary based on total vocabulary size or minimum frequency count. This file should already consist of **whitespace-separated tokens**. Use something like the Stanford Tokenizer (http://nlp.stanford.edu/software/tokenizer.shtml) first on raw text.\n",
      "2. cooccur\n",
      "Constructs word-word cooccurrence statistics from a corpus. The user should supply a vocabulary file, as produced by 'vocab_count', and may specify a variety of parameters, as described by running './cooccur'.\n",
      "3. shuffle\n",
      "Shuffles the binary file of cooccurrence statistics produced by 'cooccur'. For large files, the file is automatically split into chunks, each of which is shuffled and stored on disk before being merged and shuffled togther. The user may specify a number of parameters, as described by running './shuffle'.\n",
      "4. glove\n",
      "Train the GloVe model on the specified cooccurrence data, which typically will be the output of the 'shuffle' tool. The user should supply a vocabulary file, as given by 'vocab_count', and may specify a number of other parameters, which are described by running './glove'."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Corpus File for Embeddings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Generate a corpus from which we build the embeddings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sourcefiles = []\n",
      "sourcefiles.append(\"data/deu-at_web_2014_100K/deu-at_web_2014_100K-sentences.txt\")\n",
      "sourcefiles.append(\"data/deu-at_web_2014_10K/deu-at_web_2014_10K-sentences.txt\")\n",
      "sourcefiles.append(\"data/deu_news_2015_100K/deu_news_2015_100K-sentences.txt\")\n",
      "sourcefiles.append(\"data/deu_wikipedia_2016_100K/deu_wikipedia_2016_100K-sentences.txt\")\n",
      "sourcefiles.append(\"data/deu_wikipedia_2016_10K/deu_wikipedia_2016_10K-sentences.txt\")\n",
      "sourcefiles.append(\"data/deu_news_2015_10K/deu_news_2015_10K-sentences.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove line number and write all sentences into single file\n",
      "\n",
      "with open(corpus_for_embeddings_regular_words, 'w') as target:\n",
      "    for filename in sourcefiles:\n",
      "        with open(filename) as source:\n",
      "            for sentence in source:\n",
      "                sentence = sentence.split()\n",
      "                sentence = sentence[1:] # remove the line number at the beginning\n",
      "                sentence = \" \".join(sentence)\n",
      "                target.write(sentence + \"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Vocab Count"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmd = path_to_GloVe + \"vocab_count\"\n",
      "cmd += \" -min-count 1\"\n",
      "cmd += \" -verbose 2\"\n",
      "with open(corpus_for_embeddings_regular_words) as src:\n",
      "    with open(vocabulary_count, \"w\") as target:\n",
      "        process = subprocess.Popen(shlex.split(cmd), stdin = src, stdout = target)\n",
      "        (stdoutdata, stderrdata) = process.communicate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Cooccur"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from shutil import copyfile\n",
      "copyfile(vocabulary_count, \"vocab.txt\") # vocab file must be called vocab.txt for cooccur, other filename does not work\n",
      "\n",
      "cmd = path_to_GloVe + \"cooccur\"\n",
      "cmd += \" -verbose 2\"\n",
      "cmd += \" -symmetric 1\"\n",
      "cmd += \" -window-size 15\" # Number of context words to the left (and to the right, if symmetric = 1); default 15\n",
      "cmd += \" -memory 5.5\" # memory limit in GB, soft\n",
      "cmd += \" -overflow-file tmp_cooccur_overflow\"\n",
      "cmd += \" -vocab_file vocab.txt\"\n",
      "with open(corpus_for_embeddings_regular_words) as src:\n",
      "    with open(vocabulary_cooccur, \"w\") as target:\n",
      "        with open(\"tmp_stderr\", \"w\") as err:\n",
      "            process = subprocess.Popen(shlex.split(cmd), stdin = src, stdout = target, stderr = err)\n",
      "            (stdoutdata, stderrdata) = process.communicate()\n",
      "\n",
      "try:\n",
      "    os.remove(\"vocab.txt\")\n",
      "    os.remove(\"tmp_cooccur_overflow\")\n",
      "except:\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. Shuffle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmd = path_to_GloVe + \"shuffle\"\n",
      "cmd += \" -verbose 2\"\n",
      "cmd += \" -memory 5.5\" # memory limit in GB, soft\n",
      "cmd += \" -temp-file tmp_shuffle\"\n",
      "cmd += \" -vocab_file vocab.txt\"\n",
      "with open(vocabulary_cooccur) as src:\n",
      "    with open(vocabulary_shuffle, \"w\") as target:\n",
      "        with open(\"tmp_stderr\", \"w\") as err:\n",
      "            process = subprocess.Popen(shlex.split(cmd), stdin = src, stdout = target, stderr = err)\n",
      "            (stdoutdata, stderrdata) = process.communicate()\n",
      "\n",
      "try:\n",
      "    os.remove(\"tmp_shuffle\")\n",
      "except:\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4. Glove"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmd = path_to_GloVe + \"glove\"\n",
      "cmd += \" -verbose 2\"\n",
      "cmd += \" -write-header 1\" # If 1, write vocab_size/vector_size as first line. Do nothing if 0 (default).\n",
      "cmd += \" -vector-size \"+str(embedding_dimensions_regular_words) # Dimension of word vector representations (excluding bias term); default 50\n",
      "cmd += \" -threads 3\"\n",
      "cmd += \" -iter 100\" # Number of training iterations\n",
      "cmd += \" -eta 0.05\" # Initial learning rate\n",
      "cmd += \" -alpha 0.75\" # Parameter in exponent of weighting function; default 0.75\n",
      "cmd += \" -x-max 100.0\" # Parameter specifying cutoff in weighting function; default 100.0\n",
      "cmd += \" -binary 2\" # Save output in binary format (0: text, 1: binary, 2: both)\n",
      "cmd += \" -model 1\" # text output:\n",
      "                    # 0: output all data, for both word and context word vectors, including bias terms\n",
      "                    # 1: output word vectors, excluding bias terms\n",
      "                    # 2: output word vectors + context word vectors, excluding bias terms\n",
      "cmd += \" -input-file \" + vocabulary_shuffle # Binary input file of shuffled cooccurrence data\n",
      "cmd += \" -vocab-file \" + vocabulary_count # File containing vocabulary (truncated unigram counts, produced by 'vocab_count')\n",
      "cmd += \" -save-file \" + vocabulary_embeddings_regular_words # Filename, excluding extension, for word vector output\n",
      "cmd += \" -gradsq-file glove_gradsq\" # Filename, excluding extension, for squared gradient output \n",
      "\n",
      "with open(\"tmp_stderr\", \"w\") as err:\n",
      "    subprocess.call(shlex.split(cmd), stderr = err)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Embeddings For POS Tags and Dependency Labels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sourcefile = \"data/deu_20k/parsed.conll.modified\"\n",
      "vocabulary_embeddings_postags_deplabels = path_to_data + \"embeddings_postags_deplabels\"\n",
      "corpus_for_embeddings_postags_deplabels = path_to_data + \"embeddings_corpus_postags_deplabels.txt\"\n",
      "\n",
      "embedding_dimensions_postags_deplabels = 64"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Corpus File for Embeddings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# conll-X table row elements\n",
      "i_ID = 0\n",
      "i_FORM = 1\n",
      "i_LEMMA = 2\n",
      "i_POSTAG = 3\n",
      "i_HEAD_ID = 4\n",
      "i_DEPREL = 5\n",
      "\n",
      "with open(corpus_for_embeddings_postags_deplabels, \"w\") as target:\n",
      "    with open(sourcefile) as src:\n",
      "        sentence = []\n",
      "        for line in src:\n",
      "            line = line.strip().split()\n",
      "            if len(line) > 1:\n",
      "                sentence.append( line[i_POSTAG] )\n",
      "                sentence.append( line[i_DEPREL] )\n",
      "            else:\n",
      "                target.write( \" \".join(sentence) + \"\\n\" )\n",
      "                sentence = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Vocab Count"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmd = path_to_GloVe + \"vocab_count\"\n",
      "cmd += \" -min-count 1\"\n",
      "cmd += \" -verbose 2\"\n",
      "with open(corpus_for_embeddings_postags_deplabels) as src:\n",
      "    with open(vocabulary_count, \"w\") as target:\n",
      "        process = subprocess.Popen(shlex.split(cmd), stdin = src, stdout = target)\n",
      "        (stdoutdata, stderrdata) = process.communicate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Cooccur"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from shutil import copyfile\n",
      "copyfile(vocabulary_count, \"vocab.txt\") # vocab file must be called vocab.txt for cooccur, other filename does not work\n",
      "\n",
      "cmd = path_to_GloVe + \"cooccur\"\n",
      "cmd += \" -verbose 2\"\n",
      "cmd += \" -symmetric 1\"\n",
      "cmd += \" -window-size 15\" # Number of context words to the left (and to the right, if symmetric = 1); default 15\n",
      "cmd += \" -memory 5.5\" # memory limit in GB, soft\n",
      "cmd += \" -overflow-file tmp_cooccur_overflow\"\n",
      "cmd += \" -vocab_file vocab.txt\"\n",
      "with open(corpus_for_embeddings_postags_deplabels) as src:\n",
      "    with open(vocabulary_cooccur, \"w\") as target:\n",
      "        with open(\"tmp_stderr\", \"w\") as err:\n",
      "            process = subprocess.Popen(shlex.split(cmd), stdin = src, stdout = target, stderr = err)\n",
      "            (stdoutdata, stderrdata) = process.communicate()\n",
      "\n",
      "try:\n",
      "    os.remove(\"vocab.txt\")\n",
      "    os.remove(\"tmp_cooccur_overflow\")\n",
      "except:\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. Shuffle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmd = path_to_GloVe + \"shuffle\"\n",
      "cmd += \" -verbose 2\"\n",
      "cmd += \" -memory 5.5\" # memory limit in GB, soft\n",
      "cmd += \" -temp-file tmp_shuffle\"\n",
      "cmd += \" -vocab_file vocab.txt\"\n",
      "with open(vocabulary_cooccur) as src:\n",
      "    with open(vocabulary_shuffle, \"w\") as target:\n",
      "        with open(\"tmp_stderr\", \"w\") as err:\n",
      "            process = subprocess.Popen(shlex.split(cmd), stdin = src, stdout = target, stderr = err)\n",
      "            (stdoutdata, stderrdata) = process.communicate()\n",
      "\n",
      "try:\n",
      "    os.remove(\"tmp_shuffle\")\n",
      "except:\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4. Glove"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmd = path_to_GloVe + \"glove\"\n",
      "cmd += \" -verbose 2\"\n",
      "cmd += \" -write-header 1\" # If 1, write vocab_size/vector_size as first line. Do nothing if 0 (default).\n",
      "cmd += \" -vector-size \"+str(embedding_dimensions_postags_deplabels) # Dimension of word vector representations (excluding bias term); default 50\n",
      "cmd += \" -threads 3\"\n",
      "cmd += \" -iter 100\" # Number of training iterations\n",
      "cmd += \" -eta 0.05\" # Initial learning rate\n",
      "cmd += \" -alpha 0.75\" # Parameter in exponent of weighting function; default 0.75\n",
      "cmd += \" -x-max 100.0\" # Parameter specifying cutoff in weighting function; default 100.0\n",
      "cmd += \" -binary 2\" # Save output in binary format (0: text, 1: binary, 2: both)\n",
      "cmd += \" -model 1\" # text output:\n",
      "                    # 0: output all data, for both word and context word vectors, including bias terms\n",
      "                    # 1: output word vectors, excluding bias terms\n",
      "                    # 2: output word vectors + context word vectors, excluding bias terms\n",
      "cmd += \" -input-file \" + vocabulary_shuffle # Binary input file of shuffled cooccurrence data\n",
      "cmd += \" -vocab-file \" + vocabulary_count # File containing vocabulary (truncated unigram counts, produced by 'vocab_count')\n",
      "cmd += \" -save-file \" + vocabulary_embeddings_postags_deplabels # Filename, excluding extension, for word vector output\n",
      "cmd += \" -gradsq-file glove_gradsq\" # Filename, excluding extension, for squared gradient output \n",
      "\n",
      "with open(\"tmp_stderr\", \"w\") as err:\n",
      "    subprocess.call(shlex.split(cmd), stderr = err)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}